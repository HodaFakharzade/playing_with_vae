{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-28T14:16:23.175180Z",
     "start_time": "2018-06-28T14:16:23.172142Z"
    }
   },
   "source": [
    "![Latent vector spaces](compare.png)\n",
    "\n",
    "\n",
    "\n",
    "![FMNIST reconstructions](reconstructions.png)\n",
    "\n",
    "# Intro\n",
    "\n",
    "This is a test task I did for some reason.\n",
    "It contains evaluation of:\n",
    "- FC VAE / FCN VAE on MNIST / FMNIST for image reconstruction;\n",
    "- Comparison of embeddings produced by VAE / PCA / UMAP for classification;\n",
    "\n",
    "# TLDR\n",
    "\n",
    "What you can find here:\n",
    "- A working VAE example on PyTorch with a lot of flags (both FC and FCN, as well as a number of failed experiments);\n",
    "- Some experiment boilerplate code;\n",
    "- Comparison between embeddings produced by PCA / UMAP / VAEs (**spoiler** - VAEs win);\n",
    "- A step-by step logic of what I did in `main.ipynb`\n",
    "\n",
    "\n",
    "# Docker environment\n",
    "\n",
    "To build the docker image from the Dockerfile located in `dockerfile` please do:\n",
    "```\n",
    "cd dockerfile\n",
    "docker build -t vae_docker .\n",
    "```\n",
    "(you can replace public ssh key with yours, ofc)\n",
    "\n",
    "Also please make sure that [nvidia-docker2](https://github.com/nvidia/nvidia-docker/wiki/Installation-(version-2.0)) and proper nvidia drivers are installed.\n",
    "\n",
    "To test the installation run\n",
    "```\n",
    "docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi\n",
    "```\n",
    "\n",
    "Then launch the container as follows:\n",
    "```\n",
    "docker run --runtime=nvidia -e NVIDIA_VISIBLE_DEVICES=0 -it -v /your/folder/:/home/keras/notebook/your_folder -p 8888:8888 -p 6006:6006 --name vae --shm-size 16G vae_docker\n",
    "```\n",
    "\n",
    "Please note that w/o `--shm-size 16G` PyTorch dataloader classes will not work.\n",
    "The above command will start a container with a Jupyter notebook server available via port `8888`. \n",
    "Port `6006` is for tensorboard, if necessary.\n",
    "\n",
    "Then you can exec into the container like this. All the scripts were run as root, but they must also work under user `keras`\n",
    "```\n",
    "docker exec -it --user root REPLACE_WITH_CONTAINER_ID /bin/bash\n",
    "```\n",
    "or\n",
    "```\n",
    "docker exec -it --user keras REPLACE_WITH_CONTAINER_ID /bin/bash\n",
    "```\n",
    "\n",
    "To find out the container ID run\n",
    "```\n",
    " docker container ls\n",
    "```\n",
    "\n",
    "\n",
    "# Most important dependencies (if you do not want docker)\n",
    "\n",
    "These are the most important dependencies (others you can just install in the progress):\n",
    "```\n",
    "Ubuntu 16.04\n",
    "cuda 9.0\n",
    "cudnn 7\n",
    "python 3.6\n",
    "pip\n",
    "PIL\n",
    "tensorflow-gpu (for tensorboard)\n",
    "pandas\n",
    "numpy\n",
    "matplotlib\n",
    "seaborn\n",
    "tqdm\n",
    "scikit-learn\n",
    "pytorch 0.4.0 (cuda90)\n",
    "torchvision 2.0\n",
    "datashader\n",
    "umap\n",
    "```\n",
    "If you have trouble with these, look up how I install them in the Dockerfile / jupyter notebook.\n",
    "\n",
    "\n",
    "# Results\n",
    "\n",
    "## VAE\n",
    "\n",
    "**The best model can be trained as follows**\n",
    "\n",
    "```\n",
    "python3 train.py \\\n",
    "\t--epochs 30 --batch-size 512 --seed 42 \\\n",
    "\t--model_type fc_conv --dataset_type fmnist --latent_space_size 10 \\\n",
    "\t--do_augs False \\\n",
    "\t--lr 1e-3 --m1 40 --m2 50 \\\n",
    "\t--optimizer adam \\\n",
    "\t--do_running_mean False --img_loss_weight 1.0 --kl_loss_weight 1.0 \\\n",
    "\t--image_loss_type bce --ssim_window_size 5 \\\n",
    "\t--print-freq 10 \\\n",
    "\t--lognumber fmnist_fc_conv_l10_rebalance_no_norm \\\n",
    "\t--tensorboard True --tensorboard_images True \\\n",
    "```\n",
    "\n",
    "If you launch this code, the copy of `FMNIST` dataset will be dowloaded automatically.\n",
    "\n",
    "Suggested alternative values for the flags for playing with them:\n",
    "- `dataset_type` - can be set to `mnist` and `fmnist`. In each case will download the necessary dataset\n",
    "- `latent_space_size` - will affect the latent space in combination with `model_type` `fc_conv` or `fc`. Other model types do not work properly\n",
    "- `m1` and `m2` control lr decay, but it did not really help here\n",
    "- `image_loss_type` can be set to `bce`, `mse` or `ssim`. In practice `bce` works best. `mse` is worse. I suppose that proper scaling is required to make it work with `ssim` (it does not train now)\n",
    "- `tensorboard`  and `tensorboard_images` can also be set to `False`. But they just write logs, so you may just not bother\n",
    "\n",
    "These flags are optional `--tensorboard True --tensorboard_images True`, in order to use them, you have to \n",
    "- install tensorboard (installs with tensorflow)\n",
    "- launch tensorboard with the following command `tensorboard --logdir='path/to/tb_logs' --port=6006`\n",
    "\n",
    "You can also resume from the best checkpoint using these flags:\n",
    "```\n",
    "python3 train.py \\\n",
    "\t--resume weights/fmnist_fc_conv_l10_rebalance_no_norm_best.pth.tar \\\n",
    "\t--epochs 60 --batch-size 512 --seed 42 \\\n",
    "\t--model_type fc_conv --dataset_type fmnist --latent_space_size 10 \\\n",
    "\t--do_augs False \\\n",
    "\t--lr 1e-3 --m1 50 --m2 100 \\\n",
    "\t--optimizer adam \\\n",
    "\t--do_running_mean False --img_loss_weight 1.0 --kl_loss_weight 1.0 \\\n",
    "\t--image_loss_type bce --ssim_window_size 5 \\\n",
    "\t--print-freq 10 \\\n",
    "\t--lognumber fmnist_resume \\\n",
    "\t--tensorboard True --tensorboard_images True \\\n",
    "```\n",
    "\n",
    "The best reconstructions are supposed to look like this (top row - original images, bottow row - reconstructions):\n",
    "![](reconstructions.png)\n",
    "\n",
    "**Brief ablation analysis of the results**\n",
    "\n",
    "**✓ What worked**\n",
    "1. Using BCE loss + KLD loss\n",
    "2. Converting a plain FC model into a conv model in the most straight-forward fashion possible, i.e. replacing this\n",
    "```\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, latent_space_size)\n",
    "        self.fc22 = nn.Linear(400, latent_space_size)\n",
    "        self.fc3 = nn.Linear(latent_space_size, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "```        \n",
    "with this\n",
    "```\n",
    "        self.fc1 = nn.Conv2d(1,32, kernel_size=(28,28), stride=1, padding=0)\n",
    "        self.fc21 = nn.Conv2d(32,latent_space_size, kernel_size=(1,1), stride=1, padding=0)\n",
    "        self.fc22 = nn.Conv2d(32,latent_space_size, kernel_size=(1,1), stride=1, padding=0)\n",
    "        \n",
    "        self.fc3 = nn.ConvTranspose2d(latent_space_size,118, kernel_size=(1,1),  stride=1, padding=0)\n",
    "        self.fc4 = nn.ConvTranspose2d(118,1, kernel_size=(28,28),  stride=1, padding=0)\n",
    "```        \n",
    "3. Using `SSIM` as visualization metric. It correlates awesomely with perceived visual similarity of the image and its reconstruction\n",
    "\n",
    "\n",
    "**✗ What did not work**\n",
    "1. Extracting `mean` and `std` from images - removing this feature boosted SSIM on FMNIST 4-5x\n",
    "2. Doing any simple augmentations (unsurprisingly - it adds a complexity level to a simple task)\n",
    "3. Any architectures beyond the most obvious ones:\n",
    "    - UNet inspired architectures (my speculation - this is because image size is very small, and very global features work best, i.e. feature extraction cascade is overkill)\n",
    "    - I tried various combinations of convolution weights, all of them did not work\n",
    "    - 1xN convolutions\n",
    "4. `MSE` loss performed poorly, `SSIM` loss did not work at all\n",
    "5. LR decay, as well as any LR besides `1e-3` (with adam) does not really help\n",
    "6. Increasing latent space to `20` or `100` does not really change much\n",
    "\n",
    "** ¯|_(ツ)_/¯ What I did not try**\n",
    "1. Ensembling or building meta-architectures\n",
    "2. Conditional VAEs\n",
    "3. Increasing network capacity\n",
    "\n",
    "## PCA vs. UMAP vs. VAE\n",
    "\n",
    "Please refer to section 5 of the `main.ipynb`\n",
    "\n",
    "Is notable that:\n",
    "- VAEs visually worked better than PCA;\n",
    "- Using the VAE embedding for classification produces higher accuracty (~80% vs. 73%);\n",
    "- A similar accuracy on train/val can be obtained using [UMAP](https://github.com/lmcinnes/umap);\n",
    "\n",
    "Jupyter notebook (.ipynb file) is best viewed using these Jupiter notebook extensions (installed with the below command, then to be turned on in the **Jupyter GUI**)\n",
    "\n",
    "```\n",
    "pip install git+https://github.com/ipython-contrib/jupyter_contrib_nbextensions\n",
    "# conda install html5lib==0.9999999\n",
    "jupyter contrib nbextension install --system\n",
    "```    \n",
    "Sometims there is a `html5lib` conflict.\n",
    "Excluded from the Dockerfile because of this conflict (sometimes occurs, sometimes not).\n",
    "![](extensions.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
